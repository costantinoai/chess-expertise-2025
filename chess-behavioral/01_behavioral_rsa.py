#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Behavioral Representational Similarity Analysis (RSA)

This script analyzes behavioral similarity judgments from the 1-back task
performed during fMRI scanning. Participants indicated preferences between
consecutive chess board stimuli, and this script:

1. Loads trial-level data from all participants
2. Converts each participant's preferences to pairwise comparisons
3. Aggregates pairwise comparisons across participants (sums counts for each pair)
4. Computes group-level behavioral RDMs for experts and novices from aggregated data
5. Correlates behavioral RDMs with theoretical model RDMs:
   - Checkmate status (checkmate vs non-checkmate)
   - Strategy type (5 different chess strategies)
   - Visual similarity (perceptual features)
6. Saves all intermediate results for visualization

IMPORTANT: Individual RDMs/DSMs are NOT computed or averaged. Instead, raw pairwise
preferences are aggregated across participants, and the group RDM/DSM is computed
from the aggregated counts. This preserves all information and is more efficient.

NO PLOTTING: Figures are generated by 91_plot_behavioral_panels.py and tables by
81_table_behavioral_correlations.py

Outputs (saved to results/<timestamp>_behavioral_rsa/):
    - expert_behavioral_rdm.npy: Expert group RDM
    - novice_behavioral_rdm.npy: Novice group RDM
    - expert_directional_dsm.npy: Expert directional preference matrix
    - novice_directional_dsm.npy: Novice directional preference matrix
    - pairwise_data.pkl: Aggregated pairwise comparison DataFrames
    - model_rdms.pkl: Model RDM matrices
    - correlation_results.pkl: Correlation statistics
    - correlation_summary.csv: Summary table (human-readable)

Analysis 1 from manuscript: Main Fig 2, Methods Sec 3.5.3

Key Result: Experts' preferences correlate with Checkmate (r=0.49) and
Strategy (r=0.20); Novices show no significant correlations.
"""

import sys
import os
import numpy as np
import pandas as pd
from pathlib import Path
import pickle

# Add parent (repo root) to sys.path for 'common'
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import from common utilities
from common import (
    CONFIG,
    setup_analysis,
    log_script_end,
    get_participants_with_expertise,
    load_stimulus_metadata,
    MODEL_LABELS,
)
from common.stats_utils import apply_fdr_correction

# Import from behavioral modules (now accessible via sys.path)
from modules.data_loading import load_participant_trial_data
from modules.rdm_utils import (
    create_pairwise_df,
    compute_symmetric_rdm,
    compute_directional_dsm,
    correlate_with_all_models,
    aggregate_pairwise_counts,
)

# ============================================================================
# Configuration & Setup
# ============================================================================

# All-in-one setup: creates output dir, logging, sets random seed, suppresses warnings
config, output_dir, logger = setup_analysis(
    analysis_name="behavioral_rsa",
    results_base=Path("results"),
    script_file=__file__,
)

# ============================================================================
# Helper Functions
# ============================================================================


def analyze_group(
    group_pairwise: pd.DataFrame,
    category_df: pd.DataFrame,
    expertise_label: str
) -> tuple:
    """
    Perform complete analysis pipeline for a single group (experts or novices).

    This function computes behavioral RDMs and correlates them with model RDMs.
    NO PLOTTING - all visualization is done in separate 02_plot_behavioral_results.py

    This function:
    1. Takes aggregated pairwise data (already summed across participants)
    2. Computes behavioral RDM from aggregated counts
    3. Computes directional DSM from aggregated counts
    4. Correlates RDM with model RDMs

    Parameters
    ----------
    group_pairwise : pd.DataFrame
        Aggregated pairwise comparison data with columns: better, worse, count
    category_df : pd.DataFrame
        Stimulus categories for model RDMs
    expertise_label : str
        'Experts' or 'Novices'

    Returns
    -------
    group_rdm : np.ndarray
        Symmetric behavioral RDM for the group
    group_dsm : np.ndarray
        Directional DSM for the group
    correlation_results : list
        Correlation results with each model
    model_rdms : list
        Model RDM matrices (for plotting)
    """
    logger.info(f"Analyzing {expertise_label} group...")
    logger.info(f"  {len(group_pairwise)} unique pairwise comparisons")

    # Convert aggregated pairwise preference counts into a symmetric dissimilarity matrix.
    # For each pair (i,j), dissimilarity reflects how inconsistent preferences were:
    # - Low dissimilarity: participants consistently preferred one stimulus over the other
    # - High dissimilarity: preferences were random or split
    # The RDM is symmetric because we don't care about direction, only consistency.
    group_rdm = compute_symmetric_rdm(group_pairwise)
    logger.info("  Computed behavioral RDM")

    # Also create a directional dissimilarity matrix that preserves which stimulus was preferred.
    # DSM(i,j) = proportion of times i was preferred over j. This is asymmetric: DSM(i,j) ≠ DSM(j,i).
    # Useful for understanding directional preference biases (e.g., always prefer check positions).
    group_dsm = compute_directional_dsm(group_pairwise)
    logger.info("  Computed directional DSM")

    # Test whether behavioral preferences align with theoretical models of similarity.
    # For each model (check status, strategy, visual appearance), compute Pearson correlation
    # between behavioral RDM and model RDM. Bootstrap resampling (10,000 iterations; percentile
    # method via pingouin) provides 95% confidence intervals and parametric p-values, testing
    # whether the correlation significantly differs from zero.
    model_columns = CONFIG['MODEL_COLUMNS']
    correlation_results, model_rdms = correlate_with_all_models(
        group_rdm, category_df, model_columns
    )

    # Log results for each model
    logger.info(f"\n  Correlation Results ({expertise_label}):")
    for col, r, p, ci_l, ci_u in correlation_results:
        logger.info(f"    {col}: r={r:.3f}, p={p:.3e}, 95%CI=[{ci_l:.3f}, {ci_u:.3f}]")

    return group_rdm, group_dsm, correlation_results, model_rdms


# ============================================================================
# Load Participants
# ============================================================================

logger.info("Loading participant information...")
participants_list, (n_experts, n_novices) = get_participants_with_expertise()
logger.info(f"Loaded {n_experts} experts and {n_novices} novices")

# ============================================================================
# Load Trial Data and Create Pairwise Comparisons for All Participants
# ============================================================================

logger.info("Loading trial data and creating pairwise comparisons...")

# We'll store individual pairwise DataFrames (one per participant) before aggregating.
# This preserves the complete data structure needed for bootstrap resampling later.
experts_pairwise_dfs = []
novices_pairwise_dfs = []

# Process each participant individually
for subject_id, is_expert in participants_list:
    logger.info(f"  Processing {subject_id} (expert={is_expert})...")

    # Read BIDS events.tsv files containing trial-by-trial task data.
    # The 1-back task presents stimuli sequentially; participants indicate which of the
    # two most recent boards they prefer. We extract these preferences as trial records.
    trial_df = load_participant_trial_data(subject_id, is_expert, CONFIG['BIDS_ROOT'])

    if trial_df is None:
        logger.warning(f"  Skipping {subject_id} (no valid data)")
        continue

    # Convert trial-level preferences into pairwise comparison format.
    # Each trial where participant prefers stimulus A over stimulus B becomes a record:
    # (better=A, worse=B). This is the fundamental unit for RSA analysis.
    pairwise_df = create_pairwise_df(trial_df)

    # Store by group for later aggregation
    if is_expert:
        experts_pairwise_dfs.append(pairwise_df)
    else:
        novices_pairwise_dfs.append(pairwise_df)

logger.info(
    f"Collected pairwise data from {len(experts_pairwise_dfs)} experts and {len(novices_pairwise_dfs)} novices"
)

# ============================================================================
# Aggregate Pairwise Data Across Participants
# ============================================================================
logger.info("Aggregating pairwise preferences across participants...")

# Instead of computing individual RDMs and averaging (which loses information),
# we aggregate raw counts across participants. For each unique pair (i,j), sum the
# number of times i was preferred over j across all participants in the group.
# This preserves all information and avoids aggregation artifacts.
expert_pairwise = aggregate_pairwise_counts(experts_pairwise_dfs)
logger.info(f"  Experts: {len(expert_pairwise)} unique stimulus pairs")

novice_pairwise = aggregate_pairwise_counts(novices_pairwise_dfs)
logger.info(f"  Novices: {len(novice_pairwise)} unique stimulus pairs")

# ============================================================================
# Load Stimulus Categories (Once for Both Groups)
# ============================================================================
logger.info("Loading stimulus categories for model RDMs...")
category_df = load_stimulus_metadata()
logger.info(f"Loaded categories for {len(category_df)} stimuli")

# ============================================================================
# Analyze Groups
# ============================================================================

expert_rdm, expert_dsm, expert_correlations, expert_model_rdms = analyze_group(
    expert_pairwise, category_df, "Experts"
)

novice_rdm, novice_dsm, novice_correlations, novice_model_rdms = analyze_group(
    novice_pairwise, category_df, "Novices"
)

# ============================================================================
# Save All Results
# ============================================================================

logger.info("Saving all results...")

# Save RDMs and DSMs
np.save(output_dir / "expert_behavioral_rdm.npy", expert_rdm)
np.save(output_dir / "novice_behavioral_rdm.npy", novice_rdm)
np.save(output_dir / "expert_directional_dsm.npy", expert_dsm)
np.save(output_dir / "novice_directional_dsm.npy", novice_dsm)
logger.info("  Saved behavioral RDMs and DSMs")

# Compute 2D multidimensional scaling (MDS) embeddings for visualization.
# MDS finds a low-dimensional (2D here) representation that preserves pairwise dissimilarities
# as closely as possible. This allows us to visualize the 40×40 RDM as a 2D scatter plot,
# where distance between points approximates dissimilarity. Useful for identifying clusters
# and visually comparing expert vs novice representational spaces.
from sklearn.manifold import MDS
mds = MDS(n_components=2, dissimilarity="precomputed", random_state=CONFIG['RANDOM_SEED'])
logger.info("Computing 2D MDS embeddings for Experts and Novices (for plotting)...")
expert_mds_coords = mds.fit_transform(expert_rdm)
novice_mds_coords = mds.fit_transform(novice_rdm)
np.save(output_dir / "expert_mds_coords.npy", expert_mds_coords)
np.save(output_dir / "novice_mds_coords.npy", novice_mds_coords)
logger.info("  Saved MDS coordinates for both groups")

# Save pairwise data
pairwise_data = {
    "expert_pairwise": expert_pairwise, 
    "novice_pairwise": novice_pairwise
}
with open(output_dir / "pairwise_data.pkl", "wb") as f:
    pickle.dump(pairwise_data, f)
logger.info("  Saved pairwise comparison data")

# Package model RDMs and correlation results for downstream plotting/tables.
# Model RDMs are saved as a dictionary (model_name -> 40×40 matrix) for easy lookup.
# Correlation results include all statistics: r, p-value, and 95% confidence intervals.
model_rdms_dict = {
    col: rdm for (col, _, _, _, _), rdm in zip(expert_correlations, expert_model_rdms)
}

# Compute FDR-corrected p-values across model tests per group
model_columns = CONFIG['MODEL_COLUMNS']
exp_p_raw = np.array([p for (_, _, p, _, _) in expert_correlations], dtype=float)
nov_p_raw = np.array([p for (_, _, p, _, _) in novice_correlations], dtype=float)

_, exp_p_fdr = apply_fdr_correction(exp_p_raw, alpha=CONFIG.get('ALPHA_FDR', 0.05))
_, nov_p_fdr = apply_fdr_correction(nov_p_raw, alpha=CONFIG.get('ALPHA_FDR', 0.05))

expert_p_fdr_map = {name: float(pf) for (name, _, _, _, _), pf in zip(expert_correlations, exp_p_fdr)}
novice_p_fdr_map = {name: float(pf) for (name, _, _, _, _), pf in zip(novice_correlations, nov_p_fdr)}

correlation_results = {
    "expert": expert_correlations,
    "novice": novice_correlations,
    "expert_p_fdr": expert_p_fdr_map,
    "novice_p_fdr": novice_p_fdr_map,
    "model_columns": model_columns,
    "alpha_fdr": CONFIG.get('ALPHA_FDR', 0.05),
    "fdr_method": "fdr_bh",
}

with open(output_dir / "model_rdms.pkl", "wb") as f:
    pickle.dump(model_rdms_dict, f)
with open(output_dir / "correlation_results.pkl", "wb") as f:
    pickle.dump(correlation_results, f)
logger.info("  Saved model RDMs and correlation results")

# Create a human-readable summary table comparing expert and novice correlations.
# This CSV file provides a quick overview without needing to load pickle files.
# Table includes: model names, expert r/p/CI, novice r/p/CI, formatted for readability.
from common.report_utils import format_correlation_summary

model_columns = CONFIG['MODEL_COLUMNS']
labels_map = MODEL_LABELS
summary_df = format_correlation_summary(
    expert_correlations,
    novice_correlations,
    model_columns=model_columns,
    model_labels_map=labels_map,
    exp_p_fdr=expert_p_fdr_map,
    nov_p_fdr=novice_p_fdr_map,
)
summary_df.to_csv(output_dir / "correlation_summary.csv", index=False)
logger.info("  Saved summary table")

# Print summary to console
logger.info("\n" + "=" * 80)
logger.info("BEHAVIORAL RDM CORRELATIONS (Experts vs. Novices)")
logger.info("=" * 80)
logger.info("\n" + summary_df.to_string(index=False))
logger.info("=" * 80 + "\n")

# ============================================================================
# Finish
# ============================================================================

log_script_end(logger)
logger.info(f"All outputs saved to: {output_dir}")
